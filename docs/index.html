<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Comparative Evaluation of Transformer-Based Vision Encoder-Decoder Models for Brazilian Portuguese Image Captioning">
  <meta name="keywords" content="Vision Encoder-Decoder, Image Captioning, Brazilian Portuguese">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A Comparative Evaluation of Transformer-Based Vision Encoder-Decoder Models for Brazilian Portuguese Image Captioning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="../index.html">
      <span class="icon"><i class="fas fa-home"></i>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="index.html">
            A Comparative Evaluation of Transformer-Based Vision Encoder-Decoder Models for Brazilian Portuguese Image Captioning
          </a>
        </div>
      </div>
    </div>
  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Comparative Evaluation of Transformer-Based Vision Encoder-Decoder Models for 
            <span style="color: green">Bra</span><span style="color: rgb(228, 167, 0)">zil</span><span style="color: rgb(2, 2, 197)">ian</span>
            Portuguese Image Captioning
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://gabrielmotablima.github.io">Gabriel Bromonschenkel</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Xp4tZ0cAAAAJ&hl=en">Hilário Oliveira</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=AuizbbAAAAAJ&hl=en">Thiago Meireles Paixão</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Programa de Pós-Graduação em Computação Aplicada - PPComp</span> <br>
            <span class="author-block">Instituto Federal do Espírito Santo, Serra, Brazil - IFES</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://sibgrapi.sid.inpe.br/col/sid.inpe.br/sibgrapi/2024/09.01.17.48/doc/bromonschenkel-31.pdf"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/laicsiifes/ved-transformer-caption-ptbr/"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/laicsiifes/ved-for-brazilian-portuguese-ic-66d6280c9e7dbd3be32d2770"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fa fa-flask"></i>
                  </span>
                  <span>Resources</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser iamge-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/architecture.png">
      <h2 class="subtitle has-text-centered">
        Example of Transformer-based Vision Encoder-Decoder Architecture 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser iamge -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!--- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image captioning refers to the process of creating a natural language description for one 
            or more images. This task has several practical applications, from aiding in medical diagnoses 
            through image descriptions to promoting social inclusion by providing visual context to people 
            with impairments.
          </p>
          <p>
            Despite recent progress, especially in English, low-resource languages like Brazilian Portuguese 
            face a shortage of datasets, models, and studies. This work seeks to contribute to this context 
            by fine-tuning and investigating the performance of vision language models based on the 
            Transformer architecture in Brazilian Portuguese. We leverage pre-trained vision model 
            checkpoints (ViT, Swin, and DeiT) and neural language models (BERTimbau, DistilBERTimbau, and 
            GPorTuguese-2). Several experiments were carried out to compare the efficiency of different model 
            combinations using the #PraCegoVer-63K, a native Portuguese dataset, and a translated 
            version of the Flickr30K dataset.
          </p>
          <p>
            The experimental results demonstrated that configurations using the Swin, DistilBERTimbau, and 
            GPorTuguese-2 models generally achieved the best outcomes. Furthermore, the #PraCegoVer-63K 
            dataset presents a series of challenges, such as descriptions made up of multiple sentences 
            and the presence of proper names of places and people, which significantly decrease the 
            performance of the investigated models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Paper Intro -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <p>
            Our main contributions are summarized as follows:
            <ol>
              <li>
                This is the <strong>first work to conduct a comprehensive 
                experimental investigation on a fully Transformer-based vision encoder-decoder 
                architecture for Brazilian Portuguese Image Captioning</strong>. We provide a performance
                comparison of different options to encoder and decoder models.
              </li> 
              <li>
                Our extensive evaluation was conducted on two datasets: 
                the <strong>native Portuguese #PraCegoVer-63K</strong> and our </strong>translated version of the traditional 
                Flickr30K</strong>.
              </li>
              <li>
                Our source code, Portuguese translated version of
                Flickr30K, and the models that achieved the highest
                performance <strong>are publicly available</strong>.
              </li>
            </ol>
          </p>
      </div>
    </div>
  </div>
</div>
</section>


<!-- Related Work -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Related Work</h2>
          <div class="level-set has-text-justified">
            <p>
              The main advantage of our work compared to related studies lies in its comprehensive and 
              detailed approach to Image Captioning (IC) in Brazilian Portuguese, a low-resource 
              language. Unlike previous research focusing on generic models or translated datasets, 
              our study explores both translated datasets and native ones (#PraCegoVer), enabling a 
              deeper analysis of the linguistic and cultural nuances of Brazilian Portuguese. 
              Additionally, by combining advanced visual encoders (ViT, Swin, DeiT) with  
              textual decoders (BERTimbau, DistilBERTimbau, GPorTuguese-2), we identified the most 
              effective combinations for specific scenarios. Our work is also pioneering in its 
              extensive evaluation of model performance using a diverse set of metrics, including BLEU, 
              ROUGE, METEOR, CIDEr, and BERTScore, offering a broad and quantitative view of 
              performance across different contexts. This provides a solid foundation for future research 
              and advancements in IC for Brazilian Portuguese.
            </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small" style="overflow-x: auto;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="subtitle has-text-centered is-6">
            Our work compared to other Brazilian Portuguese IC works.
          </h2>
          <table>
            <thead>
                <tr>
                    <th><strong>Author(s)</strong></th>
                    <th width="13%"><strong>Fully Transformer-Based</strong></th>
                    <th width="13%"><strong>Leverage Pre-trained Checkpoints</strong></th>
                    <th width="13%"><strong>Compare Several Models</strong></th>
                    <th width="13%"><strong>Brazilian Portuguese Dataset</strong></th>
                    <th width="13%"><strong>Translated Dataset</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong> Santos et. al [1]</strong></td>
                    <td>❌</td>
                    <td>❌</td>
                    <td>❌</td>
                    <td>✅</td>
                    <td>❌</td>
                </tr>
            </tbody>
            <tbody>
                <tr>
                  <td><strong> Gondim et. al [2]</strong></td>
                  <td>❌</td>
                  <td>❌</td>
                  <td>❌</td>
                  <td>❌</td>
                  <td>✅</td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td><strong> Alencar et. al [2]</strong></td>
                    <td>✅</td>
                    <td>❌</td>
                    <td>❌</td>
                    <td>❌</td>
                    <td>✅</td>
                </tr>
                <tr>
                    <td><strong>Ours</strong></td>
                    <td>✅</td>
                    <td>✅</td>
                    <td>✅</td>
                    <td>✅</td>
                    <td>✅</td>
                </tr>
              </tbody>
            </tbody>
          </table>
          <br>
          <img src="static/images/references.png">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- How To Use -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Experimental Results</h2>
          <p>
            The models with highest evaluation metrics are available on 
            <a href="https://huggingface.co/collections/laicsiifes/ved-for-brazilian-portuguese-ic-66d6280c9e7dbd3be32d2770" target="_blank">Hugging Face</a>.
          </p>
          <p>
            Here, the language models names stand for their portuguese version: BERT for BERTimbau, DistilBERT for DistilBERTimbau, and 
            GPT-2 for GPorTuguese-2.
          </p>
          <h2 class="title is-4">Flickr30K Portuguese Dataset</h2>
          <div class="level-set has-text-justified">
          <br>

          <p>
            All the models with Swin Transformer as encoder achieved the best results in evaluation metrics. The best one 
            is the model that used Swin together with the portuguese version of DistilBERT, which surpassed its teacher
            model (the portuguese version of BERT).
          </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small" style="overflow-x: auto;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="subtitle has-text-centered is-6">
            Evaluation results (%) for the Flick30K dataset. The three higher are bold, and the highest 
            have green background.
          </h2>
          <table style="overflow: scroll;">
            <thead>
                <tr>
                    <th><strong>Encoder</strong></th>
                    <th><strong>Decoder</strong></th>
                    <th><strong>CIDEr-D</strong></th>
                    <th><strong>BLEU-4</strong></th>
                    <th><strong>ROUGE-L</strong></th>
                    <th><strong>METEOR</strong></th>
                    <th><strong>BERTScore</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td></td>
                    <td><strong>BERT<sub>BASE</sub></strong></td>
                    <td>49.53</td>
                    <td>19.20</td>
                    <td>36.00</td>
                    <td>39.80</td>
                    <td>69.58</td>
                </tr>
            </tbody>
            <tbody>
                <tr>
                    <td><strong>DeiT<sub>BASE</sub></strong></td>
                    <td><strong>DistilBERT<sub>BASE</sub></strong></td>
                    <td>50.58</td>
                    <td>19.24</td>
                    <td>35.77</td>
                    <td>39.93</td>
                    <td>69.50</td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td></td>
                    <td><strong>GPT-2<sub>SMALL</sub></strong></td>
                    <td>50.61</td>
                    <td>19.83</td>
                    <td>36.30</td>
                    <td>40.52</td>
                    <td>69.66</td>
                </tr>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"></td>
                    <td><strong>BERT<sub>BASE</sub></strong></td>
                    <td><strong>62.42</strong></td>
                    <td><strong>22.78</strong></td>
                    <td><strong>38.71</strong></td>
                    <td><strong>43.47</strong></td>
                    <td><strong>71.19</strong></td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>Swin<sub>BASE</sub></strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>DistilBERT<sub>BASE</sub></strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>66.73</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>24.65</strong></td> 
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>39.98</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>44.71</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>72.30</strong></td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"></td>
                    <td><strong>GPT-2<sub>SMALL</sub></strong></td>
                    <td><strong>64.71</strong></td>
                    <td><strong>23.15</strong></td>
                    <td><strong>39.39</strong></td>
                    <td><strong>44.36</strong></td>
                    <td><strong>71.70</strong></td>
                </tr>
                <tr>
                  <td></td>
                  <td><strong>BERT<sub>BASE</sub></strong></td>
                  <td>57.32</td>
                  <td>22.12</td>
                  <td>37.50</td>
                  <td>41.72</td>
                  <td>70.63</td>
              </tr>
            </tbody>
            <tbody>
              <tr>
                  <td><strong>ViT<sub>BASE</sub></strong></td>
                  <td><strong>DistilBERT<sub>BASE</sub></strong></td>
                  <td>59.32</td>
                  <td>21.19</td>
                  <td>37.74</td>
                  <td>42.70</td>
                  <td>71.15</td>
              </tr>
            </tbody>
            <tbody>
              <tr>
                  <td></td>
                  <td><strong>GPT-2<sub>SMALL</sub></strong></td>
                  <td>59.02</td>
                  <td>21.39</td>
                  <td>37.68</td>
                  <td>42.64</td>
                  <td>71.03</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <p>
          It is worth to point out that Flickr30K has shorter captions and lower caption variance when compared 
          to another datasets like #PraCegoVer. The scenes are generic and depict few perspectives, and the descriptions 
          are way simple.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small" style="overflow-x: auto;">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <img src="static/images/results_flickr.png">
          <h2 class="subtitle has-text-centered is-6">
            Left image, where Swin-DistilBERT achieved lower performance. Right image, where Swin-DistilBERT 
            achieved higher performance. The side comments are about the dataset overall characteristics.
          </h2>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <h2 class="title is-4">#PraCegoVer-63K Dataset</h2>
          <p>
            All the models with GPT-2 as decoder achieved the best results in evaluation metrics. The best one 
            is the model that used Swin together with the portuguese version of GPT-2.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small" style="overflow-x: auto;">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <h2 class="subtitle has-text-centered is-6">
            Evaluation results (%) for the #PraCegoVer-63K dataset. The three higher are bold, and the highest 
            have green background.
          </h2>
          <table style="overflow: scroll;">
            <thead>
                <tr>
                    <th><strong>Encoder</strong></th>
                    <th><strong>Decoder</strong></th>
                    <th><strong>CIDEr-D</strong></th>
                    <th><strong>BLEU-4</strong></th>
                    <th><strong>ROUGE-L</strong></th>
                    <th><strong>METEOR</strong></th>
                    <th><strong>BERTScore</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td></td>
                    <td><strong>BERT<sub>BASE</sub></strong></td>
                    <td>0.99</td>
                    <td>0.00</td>
                    <td>4.02</td>
                    <td>3.49</td>
                    <td>36.20</td>
                </tr>
            </tbody>
            <tbody>
                <tr>
                    <td><strong>DeiT<sub>BASE</sub></strong></td>
                    <td><strong>DistilBERT<sub>BASE</sub></strong></td>
                    <td>1.59</td>
                    <td>0.11</td>
                    <td>9.22</td>
                    <td>7.74</td>
                    <td>45.36</td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td></td>
                    <td><strong>GPT-2<sub>SMALL</sub></strong></td>
                    <td><strong>5.95</strong></td>
                    <td><strong>1.00</strong></td>
                    <td><strong>12.44</strong></td>
                    <td><strong>13.87</strong></td>
                    <td><strong>49.11</strong></td>
                </tr>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"></td>
                    <td><strong>BERT<sub>BASE</sub></strong></td>
                    <td>1.29</td>
                    <td>0.00</td>
                    <td>4.53</td>
                    <td>3.90</td>
                    <td>30.84</td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>Swin<sub>BASE</sub></strong></td>
                    <td><strong>DistilBERT<sub>BASE</sub></strong></td>
                    <td>0.31</td>
                    <td>0.01</td>
                    <td>7.91</td>
                    <td>5.76</td>
                    <td>40.95</td>
                </tr>
              </tbody>
              <tbody>
                <tr>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>GPT-2<sub>SMALL</sub></strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>9.45</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>1.60</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>13.43</strong></td>
                    <td><strong>15.58</strong></td>
                    <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>49.85</strong></td>
                    
                </tr>
                <tr>
                  <td></td>
                  <td><strong>BERT<sub>BASE</sub></strong></td>
                  <td>0.83</td>
                  <td>0.00</td>
                  <td>3.03</td>
                  <td>2.61</td>
                  <td>27.69</td>
              </tr>
            </tbody>
            <tbody>
              <tr>
                  <td><strong>ViT<sub>BASE</sub></strong></td>
                  <td><strong>DistilBERT<sub>BASE</sub></strong></td>
                  <td>1.70</td>
                  <td>0.12</td>
                  <td>9.01</td>
                  <td>7.89</td>
                  <td>45.71</td>
              </tr>
            </tbody>
            <tbody>
              <tr>
                  <td></td>
                  <td><strong>GPT-2<sub>SMALL</sub></strong></td>
                  <td><strong>8.27</strong></td>
                  <td><strong>1.49</strong></td>
                  <td><strong>13.23</strong></td>
                  <td style="background-color: rgba(2, 124, 2, 0.172)"><strong>15.74</strong></td>
                  <td><strong>49.57</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <p>
            It is worth to point out that #PraCegoVer-63K has larger captions and higher caption variance when compared 
            to another datasets like Flickr30K and COCO Captions. The dataset contains proper names of people and places, 
            image-reference caption mismatching, linguistics errors and complex images which demands additional resources to 
            support smaller models on the generation of more accurate captions.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small" style="overflow-x: auto;">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
          <img src="static/images/results_pcv.png">
          <h2 class="subtitle has-text-centered is-6">
            Left image, where Swin-GPT-2 achieved lower performance. Right image, where Swin-GPT-2 
            achieved higher performance. The side comments are about the dataset overall characteristics.
          </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{bromonschenkel2024comparative,
    title={A Comparative Evaluation of Transformer-Based Vision Encoder-Decoder Models for Brazilian Portuguese Image Captioning},
    author={Bromonschenkel, Gabriel and Oliveira, Hil{\'a}rio and Paix{\~a}o, Thiago M},
    booktitle={2024 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)},
    pages={1--6},
    year={2024},
    organization={IEEE}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="text-align: center;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
